{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llAlvujkcS3a"
      },
      "source": [
        "# Installation/Setup\n",
        "In this workshop we will be building a class-based searching algorithm designed to search and find sentences in text based on a query. Before we start, select a **T4 GPU** as the runtime and run the following cells to install the necessary packages and data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-p5OtsMDcM3n",
        "outputId": "7e33f311-f042-4c7b-b83a-be6d394b32bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "--2024-10-24 14:41:34--  https://docs.google.com/uc?export=download&id=1dSl2QJhVUr93yPUnakzXqmrK5ZKBTciz\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.195.100, 74.125.195.113, 74.125.195.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.195.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1dSl2QJhVUr93yPUnakzXqmrK5ZKBTciz&export=download [following]\n",
            "--2024-10-24 14:41:34--  https://drive.usercontent.google.com/download?id=1dSl2QJhVUr93yPUnakzXqmrK5ZKBTciz&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.99.132, 2607:f8b0:400e:c07::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.99.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19338 (19K) [application/octet-stream]\n",
            "Saving to: ‘cleaned-tcn-description.txt’\n",
            "\n",
            "cleaned-tcn-descrip 100%[===================>]  18.88K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-10-24 14:41:36 (92.6 MB/s) - ‘cleaned-tcn-description.txt’ saved [19338/19338]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer, losses, InputExample\n",
        "from torch.utils.data import DataLoader\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Xo5wc0ydwYq"
      },
      "source": [
        "# Concepts behind our searching algorithm\n",
        "\n",
        "## Evaluating sentences\n",
        "The idea behind the searching algorithm is that we want to take some query and find the **most simlar** sentence to that query from a text. In order to do that, we need to have a method to compare our query to other sentences in a numerical way, so that we can rank them based on numerical simlilarity.\n",
        "\n",
        "One method of comparing sentences to our query is to convert them to vectors and calculate the **cosine similarity** between them. If you have taken Calculus 3, this may look familiar, as the cosine similarity between two vectors *u, v* is defined as:\n",
        "$$\n",
        "S_c(u,v)=\\frac{u\\cdot v}{\\|u\\| \\|v\\|}\n",
        "$$\n",
        "\n",
        "The output of the function is the cosine of the angle between the two vectors, and its range is equal to $[-1, 1]$. If two vectors are in the same direction then,\n",
        "$$\n",
        "S_c(u,v)=\\cos(0)=1\n",
        "$$\n",
        "and for vectors in the opposite direction\n",
        "$$\n",
        "S_c(u,v)=\\cos(\\pi)=-1\n",
        "$$\n",
        "\n",
        "We calculate the cosine similarity using the `cosine_similarity` function from `sklearn.metrics.pairwise`. The general input to this function is two matrices matrices of vectors and outputs a matrix corresponding to the cosine similarities of each vector.\n",
        "\n",
        "If you see the idea here, cosine similarity is a function that measures the direction between two vectors with vectors the are close in direction being close to 1. The method from here is to turn the sentences into vectors in a way that captures their similarity into a vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5tUSHP2gAv7",
        "outputId": "03203fce-91c5-412a-b716-f60762ca4b8c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 1.,  0., -1.]])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "i = np.array([1, 0])\n",
        "j = np.array([0, 1])\n",
        "# Calculates the cosine similarity of i with i, j, -i\n",
        "cosine_similarity([i], [i, j, -i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GiNKC37oLJg"
      },
      "source": [
        "## How to turn sentences into vectors\n",
        "Now for the important part. The entire structure of the algorithm depends on how we turn the sentences into vectors. We need to turn these sentences into vectors in a way that *similar* sentences correspond to vectors in *similar* directions. There are multiple approches to doing this, and they each have their own use cases. For the purpose of this workshop, we will utilize a sentence transformer from Hugging Face to do the embedding for us.\n",
        "\n",
        "The sentence transformer we are using is `paraphrase-MiniLM-L6-v2`, which converts each sentence into a vector in $\\mathbb{R}^{384}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8phiiRXdv8P"
      },
      "outputs": [],
      "source": [
        "# Two sentences about machine learning\n",
        "s1 = \"Machine learning algorithms are designed to learn from data and improve their performance over time.\"\n",
        "s2 = \"Deep learning is a subfield of machine learning that utilizes artificial neural networks with multiple layers.\"\n",
        "# A completely unrelated sentence\n",
        "s3 = \"The quick brown fox jumps over the lazy dog, showcasing a classic pangram for alphabet practice.\"\n",
        "\n",
        "s1_vector = model.encode(s1)\n",
        "s2_vector = model.encode(s2)\n",
        "s3_vector = model.encode(s3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xss9SvLIrAoU",
        "outputId": "99a2b812-d646-4ec3-ddb1-9eac50789418"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "s1 dimension: (384,)\n",
            "Similarity between s1 and s2: [0.50385654]\n",
            "Similarity between s1 and s3: [0.20394549]\n",
            "Similarity between s1 and s1: [1.0000001]\n"
          ]
        }
      ],
      "source": [
        "print(\"s1 dimension:\", s1_vector.shape)\n",
        "print(\"Similarity between s1 and s2:\", cosine_similarity([s1_vector], [s2_vector])[0])\n",
        "print(\"Similarity between s1 and s3:\", cosine_similarity([s1_vector], [s3_vector])[0])\n",
        "print(\"Similarity between s1 and s1:\", cosine_similarity([s1_vector], [s1_vector])[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHWsafbEtefJ"
      },
      "source": [
        "## Using NLP Techniques\n",
        "There are a few NLP techniques that we can use to help make the sentence transformer perform better.\n",
        "### Removing Stopwords and Converting to Lowercase\n",
        "We can remove stopwords from the sentences (\"there\", \"is\", \"and\") to leave only important keywords for the transformer to handle. We can also convert each sentence to be strictly lowercase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrqrRLOBrB3U",
        "outputId": "c5d98140-1d03-486e-9119-81fe2dcc4efe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "machine learning algorithms designed learn data improve performance time.\n",
            "deep learning subfield machine learning utilizes artificial neural networks multiple layers.\n",
            "quick brown fox jumps lazy dog, showcasing classic pangram alphabet practice.\n"
          ]
        }
      ],
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "s1 = ' '.join([word.lower() for word in s1.split() if word.lower() not in stop_words])\n",
        "s2 = ' '.join([word.lower() for word in s2.split() if word.lower() not in stop_words])\n",
        "s3 = ' '.join([word.lower() for word in s3.split() if word.lower() not in stop_words])\n",
        "print(s1)\n",
        "print(s2)\n",
        "print(s3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvl3jIXRxmeZ"
      },
      "source": [
        "### Lemmatization\n",
        "We can also apply a lemmatizer to the sentences that change each word to its root form. For example \"running\" -> \"run\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtK689-_wn75",
        "outputId": "02aac3e1-bb0d-469e-f21b-5d7e1fc7d87c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "machine learning algorithm designed learn data improve performance time.\n",
            "deep learning subfield machine learning utilizes artificial neural network multiple layers.\n",
            "quick brown fox jump lazy dog, showcasing classic pangram alphabet practice.\n"
          ]
        }
      ],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "s1 = ' '.join([lemmatizer.lemmatize(word) for word in s1.split()])\n",
        "s2 = ' '.join([lemmatizer.lemmatize(word) for word in s2.split()])\n",
        "s3 = ' '.join([lemmatizer.lemmatize(word) for word in s3.split()])\n",
        "print(s1)\n",
        "print(s2)\n",
        "print(s3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWUbx-P_yVkf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr4FUKZNykNK"
      },
      "source": [
        "# Putting it all Together\n",
        "Now that we have the foundation of our searching algorithm with sentence transformers and NLP techniques, we can construct a class for handling our data. Feel free to add any helper functions as you implement the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4OaWRhe3uwu"
      },
      "outputs": [],
      "source": [
        "class SearchAlgorithm:\n",
        "\n",
        "    def __init__(self, model_name=\"paraphrase-MiniLM-L6-v2\", model=None) -> None:\n",
        "        if model:\n",
        "            self.model = model\n",
        "        else:\n",
        "            self.model = SentenceTransformer(model_name)\n",
        "        self.stop_words = set(stopwords.words(\"english\"))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.documents = []\n",
        "        self.document_vectors = []\n",
        "        self.query_vector = None\n",
        "\n",
        "    def load_documents(self, documents: list[str]) -> None:\n",
        "        \"\"\"\n",
        "        TODO:\n",
        "        1.  Load in self.documents with the passed in documents\n",
        "        2.  Convert each document into a vector with the model\n",
        "            and store the vectors in self.document_vectors.\n",
        "        3.  Utilize NLP techniques to preprocess the documents.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def search(self, query: str, N: int = 3) -> list[str]:\n",
        "        \"\"\"\n",
        "        TODO:\n",
        "        Write an algorithm to return the N most similar sentences.\n",
        "        1.  Convert the query into a vector\n",
        "        2.  Calculate the cosine similariy of the query with sentences\n",
        "        3.  Order the sentences by similarity and return the top N (hint: np.argsort())\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjuQhMso53Qw"
      },
      "source": [
        "# Testing the Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqtI5NBA50bt"
      },
      "outputs": [],
      "source": [
        "# Example of documents related to machine learning\n",
        "# Already parsed and cleaned/unique\n",
        "\n",
        "documents = [\n",
        "    \"Artificial intelligence is transforming industries with its ability to mimic human decision-making.\",\n",
        "    \"Machine learning enables computers to learn from data and improve over time without being explicitly programmed.\",\n",
        "    \"Natural language processing (NLP) allows computers to understand, interpret, and respond to human language in meaningful ways.\",\n",
        "    \"Deep learning, a subset of machine learning, utilizes neural networks with multiple layers to extract high-level features from data.\",\n",
        "    \"Transformers, a type of deep learning model, have revolutionized natural language processing tasks with their attention mechanism.\",\n",
        "    \"Self-supervised learning allows models to learn from raw data without requiring labeled examples.\",\n",
        "    \"In reinforcement learning, agents learn by interacting with their environment and receiving rewards or penalties.\",\n",
        "    \"Supervised learning requires labeled datasets to train models, while unsupervised learning identifies patterns in data without labels.\",\n",
        "    \"Neural networks consist of layers of interconnected nodes that process information similarly to the human brain.\",\n",
        "    \"Transfer learning leverages knowledge from one domain to improve performance in another domain.\",\n",
        "    \"Text search algorithms help retrieve relevant information from vast datasets using techniques like keyword matching and semantic search.\",\n",
        "    \"Inverted indices are commonly used in search engines to quickly find documents containing specific keywords.\",\n",
        "    \"Embeddings are vector representations of words, phrases, or documents that capture their semantic meaning.\",\n",
        "    \"Cosine similarity is a metric used to measure the similarity between two vectors by calculating the cosine of the angle between them.\",\n",
        "    \"Document retrieval systems are designed to search, index, and retrieve documents relevant to user queries.\",\n",
        "    \"The bag-of-words model represents text data as a collection of words without considering the order of the words.\",\n",
        "    \"TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure used to evaluate the importance of a word in a document relative to a corpus.\",\n",
        "    \"Clustering algorithms like K-Means group similar data points together based on features or distance metrics.\",\n",
        "    \"Dimensionality reduction techniques like PCA (Principal Component Analysis) reduce the number of features while preserving important information.\",\n",
        "    \"Data augmentation techniques like rotation, flipping, and scaling are used in image processing to improve model generalization.\",\n",
        "    \"Regularization methods like L2 regularization help prevent overfitting in machine learning models by adding a penalty for large weights.\",\n",
        "    \"Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models.\",\n",
        "    \"Random forests are ensemble learning methods that combine the outputs of multiple decision trees to make predictions.\",\n",
        "    \"Support Vector Machines (SVM) classify data by finding the hyperplane that best separates different classes.\",\n",
        "    \"Bayesian networks represent probabilistic relationships among variables and can be used for reasoning under uncertainty.\",\n",
        "    \"Markov Chains model systems where the probability of transitioning from one state to another depends only on the current state.\",\n",
        "    \"I am a random sentence about biology. I don't have anything to do with the query\"\n",
        "]\n",
        "\n",
        "\n",
        "query = \"What is machine learning?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-e37yI8l-gpK"
      },
      "outputs": [],
      "source": [
        "# Number of lines to combine\n",
        "N = 3\n",
        "\n",
        "# Example of documents copy and pasted from a machine learning paper\n",
        "# Documents are partially parsed/cleaned, but are overall less organized\n",
        "with open('cleaned-tcn-description.txt', 'r') as f:\n",
        "    documents = f.readlines()\n",
        "temp = []\n",
        "for doc in documents:\n",
        "    if len(doc) <= 5:\n",
        "        continue\n",
        "    for sentence in doc.split('.'):\n",
        "        if len(sentence) <= 5:\n",
        "            continue\n",
        "        temp.append(sentence)\n",
        "documents = temp\n",
        "new_documents = []\n",
        "temp = []\n",
        "for doc in documents:\n",
        "    if len(temp) == N:\n",
        "        new_documents.append(' '.join(temp))\n",
        "        temp = []\n",
        "    temp.append(doc)\n",
        "tcn_documents = new_documents\n",
        "tcn_original_documents = documents.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SA9_gNp3-k0D",
        "outputId": "afd002b8-a374-418f-853a-7815deee3d52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of TCN Documents: 58\n",
            "Sample line: Overview\n",
            " A TCN, short for Temporal Convolutional Network, consists of dilated, causal 1D convolutional layers with the same input and output lengths  The following sections go into detail about what these terms actually mean\n"
          ]
        }
      ],
      "source": [
        "print(\"Length of TCN Documents:\", len(tcn_documents))\n",
        "print(\"Sample line:\", tcn_documents[5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAfAaycgo0kI"
      },
      "outputs": [],
      "source": [
        "# TODO:\n",
        "# Instantiate the class, load the documents, and search with the query\n",
        "# Try with both the organized documents and the TCN Paper\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F7Gb8eDlf5y"
      },
      "source": [
        "# Training the sentence transformer on custom data\n",
        "\n",
        "While the sentence transformer itself helps encode the data into vectors it might not perform as well on certain sentences compared to others. In order to help the sentence transformer perform better, we can train it on custom datasets to increase the accuracy of similarity.\n",
        "\n",
        "## Quora Dataset\n",
        "For the purpose of this workshop, we will still be utilizing a more generalized dataset, whereas for specific use cases, it is better to train the transfomer on similar sentences to that you are encoding. The quora dataset is comprised of pairs of questions and label indicating whether they are essentially the same question. In other words, `is_duplicate=True` if the two questions have strong similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAQdXbthhQqp"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import os\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "dataset = load_dataset('quora', 'en', split='train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7TUTQPmijqI",
        "outputId": "8ee027c4-4065-44bc-ef41-11fb8b29d242"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'questions': {'id': [62616, 62617], 'text': [\"How do I know if I'm good?\", \"How do I know what I'm good at?\"]}, 'is_duplicate': False}\n",
            "{'questions': {'id': [213590, 149174], 'text': ['How do I build my profile for top B-schools?', 'How do I build my profile for Harvard, Wharton, INSEAD etc.?']}, 'is_duplicate': True}\n",
            "{'questions': {'id': [416022, 416023], 'text': ['How good is new Zealand for post graduation studies especially in management? And what are the job prospects in new zealand?', 'What are the job prospects for health care management in New Zealand?']}, 'is_duplicate': False}\n"
          ]
        }
      ],
      "source": [
        "for example in dataset.shuffle(seed=42).select(range(3)):\n",
        "    print(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvOvWnEuij8u"
      },
      "outputs": [],
      "source": [
        "train_examples = []\n",
        "# Separates the two questions and provides a label for the training\n",
        "# is_duplicate=True -> 1, is_duplicate=False -> 0\n",
        "for example in dataset.shuffle(seed=42).select(range(10000)):\n",
        "    question1 = example['questions']['text'][0]\n",
        "    question2 = example['questions']['text'][1]\n",
        "    label = float(example['is_duplicate'])\n",
        "    train_examples.append(InputExample(texts=[question1, question2], label=label))\n",
        "\n",
        "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
        "train_loss = losses.MultipleNegativesRankingLoss(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287,
          "referenced_widgets": [
            "059c3a47ef62458e8f3166017820a7ce",
            "c8679cb9061546c69b021a6835fbbb9a",
            "d056fc07872643c9a882efd54210c060",
            "4d2619f96aad4d16927951a4b28b3682",
            "47a00c6b0e5544dc9f5204df4b210940",
            "15fa68c974c6456c9e087bedced20990",
            "8cf43193d1ba4373a698e40b95d9d01d",
            "870702a595a54c45949513da63821bf1",
            "f9a90891063449f5a0a3d9139e585bb3",
            "6a5cfb97c0694d0e934b1ec9d7188df7",
            "3f98bc7d93524baabc03a862556a7be4"
          ]
        },
        "id": "LxRG_yRejkJE",
        "outputId": "5a77c677-0bc7-469b-f27a-869fd58be408"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2500/2500 02:04, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.342900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.248200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.162900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.122900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.089500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "059c3a47ef62458e8f3166017820a7ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.fit(\n",
        "    train_objectives=[(train_dataloader, train_loss)],\n",
        "    epochs=4,\n",
        "    warmup_steps=100,\n",
        "    output_path='./output/training_stsbenchmark_quora',\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "059c3a47ef62458e8f3166017820a7ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8679cb9061546c69b021a6835fbbb9a",
              "IPY_MODEL_d056fc07872643c9a882efd54210c060",
              "IPY_MODEL_4d2619f96aad4d16927951a4b28b3682"
            ],
            "layout": "IPY_MODEL_47a00c6b0e5544dc9f5204df4b210940"
          }
        },
        "15fa68c974c6456c9e087bedced20990": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f98bc7d93524baabc03a862556a7be4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47a00c6b0e5544dc9f5204df4b210940": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "4d2619f96aad4d16927951a4b28b3682": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a5cfb97c0694d0e934b1ec9d7188df7",
            "placeholder": "​",
            "style": "IPY_MODEL_3f98bc7d93524baabc03a862556a7be4",
            "value": " 0/1 [00:00&lt;?, ?example/s]"
          }
        },
        "6a5cfb97c0694d0e934b1ec9d7188df7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "870702a595a54c45949513da63821bf1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cf43193d1ba4373a698e40b95d9d01d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8679cb9061546c69b021a6835fbbb9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15fa68c974c6456c9e087bedced20990",
            "placeholder": "​",
            "style": "IPY_MODEL_8cf43193d1ba4373a698e40b95d9d01d",
            "value": "Computing widget examples:   0%"
          }
        },
        "d056fc07872643c9a882efd54210c060": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_870702a595a54c45949513da63821bf1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f9a90891063449f5a0a3d9139e585bb3",
            "value": 1
          }
        },
        "f9a90891063449f5a0a3d9139e585bb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
